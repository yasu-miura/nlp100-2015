{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural language processing\n",
      "From Wikipedia, the free encyclopedia\n",
      "Natural language processing (NLP) is a field of computer science, artificial intelligence, and linguistics concerned with the interactions between computers and human (natural) languages. \n",
      "As such, NLP is related to the area of humani-computer interaction. \n",
      "Many challenges in NLP involve natural language understanding, that is, enabling computers to derive meaning from human or natural language input, and others involve natural language generation.\n"
     ]
    }
   ],
   "source": [
    "# 50. 文区切り\n",
    "import re\n",
    "\n",
    "sentences = []\n",
    "with open(\"./nlp.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        if line == \"\\n\":\n",
    "            continue\n",
    "        lines = re.sub(r\"(?P<group1>[.;:?!])( +)(?P<group3>[A-Z])\", r\"\\1\\2\\n\\3\",line)\n",
    "        sentence_list = lines.split(\"\\n\")\n",
    "        sentences += sentence_list[:len(sentence_list)-1]\n",
    "        \n",
    "for sentence in sentences[:5]:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural\n",
      "language\n",
      "processing\n",
      "\n",
      "\n",
      "From\n",
      "Wikipedia,\n",
      "the\n",
      "free\n",
      "encyclopedia\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 51. 単語の切り出し\n",
    "words = []\n",
    "for sentence in sentences[:2]:\n",
    "    word_list = sentence.split(\" \") + [\"\\n\"]\n",
    "    words += word_list\n",
    "\n",
    "for word in words:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural\tNatur\n",
      "language\tlanguag\n"
     ]
    }
   ],
   "source": [
    "# 52. ステミング\n",
    "# stemming.porterはPython3で動かない\n",
    "import stemming.porter2 as porter2\n",
    "\n",
    "for word in words[:2]:\n",
    "    print(word + \"\\t\" + porter2.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b''"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 53. Tokenization\n",
    "# XMLファイルの生成\n",
    "import subprocess\n",
    "\n",
    "command = \"java -cp \\\"/usr/local/lib/stanford-corenlp-full-2015-12-09/*\\\" -Xmx2g edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators tokenize,ssplit,pos,lemma,ner,parse,dcoref -file ./nlp.txt\"\n",
    "subprocess.check_output(command, shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural\n",
      "language\n",
      "processing\n",
      "From\n",
      "Wikipedia\n",
      ",\n",
      "the\n",
      "free\n",
      "encyclopedia\n",
      "Natural\n",
      "language\n",
      "processing\n",
      "-LRB-\n",
      "NLP\n",
      "-RRB-\n",
      "is\n",
      "a\n",
      "field\n",
      "of\n",
      "computer\n",
      "science\n",
      ",\n",
      "artificial\n",
      "intelligence\n",
      ",\n",
      "and\n",
      "linguistics\n",
      "concerned\n",
      "with\n",
      "the\n",
      "interactions\n",
      "between\n",
      "computers\n",
      "and\n",
      "human\n",
      "-LRB-\n",
      "natural\n",
      "-RRB-\n",
      "languages\n",
      ".\n",
      "As\n",
      "such\n",
      ",\n",
      "NLP\n",
      "is\n",
      "related\n",
      "to\n",
      "the\n",
      "area\n",
      "of\n",
      "humani-computer\n",
      "interaction\n",
      ".\n",
      "Many\n",
      "challenges\n",
      "in\n",
      "NLP\n",
      "involve\n",
      "natural\n",
      "language\n",
      "understanding\n",
      ",\n",
      "that\n",
      "is\n",
      ",\n",
      "enabling\n",
      "computers\n",
      "to\n",
      "derive\n",
      "meaning\n",
      "from\n",
      "human\n",
      "or\n",
      "natural\n",
      "language\n",
      "input\n",
      ",\n",
      "and\n",
      "others\n",
      "involve\n",
      "natural\n",
      "language\n",
      "generation\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "tree = ET.parse(\"./nlp.txt.out\")\n",
    "root = tree.getroot()\n",
    "for sentence in root[0][0][:3]:\n",
    "    for token in sentence[0]:\n",
    "        print(token[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural\tnatural\tJJ\n",
      "language\tlanguage\tNN\n",
      "processing\tprocessing\tNN\n",
      "From\tfrom\tIN\n",
      "Wikipedia\tWikipedia\tNNP\n",
      ",\t,\t,\n",
      "the\tthe\tDT\n",
      "free\tfree\tJJ\n",
      "encyclopedia\tencyclopedia\tNN\n",
      "Natural\tnatural\tJJ\n",
      "language\tlanguage\tNN\n",
      "processing\tprocessing\tNN\n",
      "-LRB-\t-lrb-\t-LRB-\n",
      "NLP\tnlp\tNN\n",
      "-RRB-\t-rrb-\t-RRB-\n",
      "is\tbe\tVBZ\n",
      "a\ta\tDT\n",
      "field\tfield\tNN\n",
      "of\tof\tIN\n",
      "computer\tcomputer\tNN\n",
      "science\tscience\tNN\n",
      ",\t,\t,\n",
      "artificial\tartificial\tJJ\n",
      "intelligence\tintelligence\tNN\n",
      ",\t,\t,\n",
      "and\tand\tCC\n",
      "linguistics\tlinguistics\tNNS\n",
      "concerned\tconcern\tVBN\n",
      "with\twith\tIN\n",
      "the\tthe\tDT\n",
      "interactions\tinteraction\tNNS\n",
      "between\tbetween\tIN\n",
      "computers\tcomputer\tNNS\n",
      "and\tand\tCC\n",
      "human\thuman\tJJ\n",
      "-LRB-\t-lrb-\t-LRB-\n",
      "natural\tnatural\tJJ\n",
      "-RRB-\t-rrb-\t-RRB-\n",
      "languages\tlanguage\tNNS\n",
      ".\t.\t.\n",
      "As\tas\tIN\n",
      "such\tsuch\tJJ\n",
      ",\t,\t,\n",
      "NLP\tnlp\tNN\n",
      "is\tbe\tVBZ\n",
      "related\trelate\tVBN\n",
      "to\tto\tTO\n",
      "the\tthe\tDT\n",
      "area\tarea\tNN\n",
      "of\tof\tIN\n",
      "humani-computer\thumani-computer\tJJ\n",
      "interaction\tinteraction\tNN\n",
      ".\t.\t.\n",
      "Many\tmany\tJJ\n",
      "challenges\tchallenge\tNNS\n",
      "in\tin\tIN\n",
      "NLP\tnlp\tNN\n",
      "involve\tinvolve\tVBP\n",
      "natural\tnatural\tJJ\n",
      "language\tlanguage\tNN\n",
      "understanding\tunderstanding\tNN\n",
      ",\t,\t,\n",
      "that\tthat\tWDT\n",
      "is\tbe\tVBZ\n",
      ",\t,\t,\n",
      "enabling\tenable\tVBG\n",
      "computers\tcomputer\tNNS\n",
      "to\tto\tTO\n",
      "derive\tderive\tVB\n",
      "meaning\tmeaning\tNN\n",
      "from\tfrom\tIN\n",
      "human\thuman\tJJ\n",
      "or\tor\tCC\n",
      "natural\tnatural\tJJ\n",
      "language\tlanguage\tNN\n",
      "input\tinput\tNN\n",
      ",\t,\t,\n",
      "and\tand\tCC\n",
      "others\tother\tNNS\n",
      "involve\tinvolve\tVBP\n",
      "natural\tnatural\tJJ\n",
      "language\tlanguage\tNN\n",
      "generation\tgeneration\tNN\n",
      ".\t.\t.\n"
     ]
    }
   ],
   "source": [
    "# 54. 品詞タグ付け\n",
    "for sentence in root[0][0][:3]:\n",
    "    for token in sentence[0]:\n",
    "        print(\"{}\\t{}\\t{}\".format(token[0].text, token[1].text, token[4].text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alan\n",
      "Turing\n",
      "Joseph\n",
      "Weizenbaum\n",
      "MARGIE\n",
      "Schank\n",
      "Wilensky\n",
      "Meehan\n",
      "Lehnert\n",
      "Carbonell\n",
      "Lehnert\n",
      "Jabberwacky\n",
      "Moore\n"
     ]
    }
   ],
   "source": [
    "# 55. 固有表現抽出\n",
    "for sentence in root[0][0]:\n",
    "    for token in sentence[0]:\n",
    "        if token[5].text == \"PERSON\":\n",
    "            print(token[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural language processing From Wikipedia , the free encyclopedia Natural language processing -LRB- NLP -RRB- is the free encyclopedia Natural language processing -LRB- NLP -RRB-(a field of computer science) , artificial intelligence , and linguistics concerned with the interactions between computers and human -LRB- natural -RRB- languages .\n",
      "As such , NLP is related to the area of humani-computer interaction .\n",
      "Many challenges in NLP involve natural language understanding , that is , enabling computers(computers) to derive meaning from human or natural language input , and others involve natural language generation .\n",
      "History The history of NLP generally starts in the 1950s , although work can be found from earlier periods .\n",
      "In 1950 , Alan Alan Turing(Turing) published an article titled `` Computing Machinery and Intelligence '' which proposed what is now called the Alan Turing(Turing) test as a criterion of intelligence .\n"
     ]
    }
   ],
   "source": [
    "# 56. 共参照解析\n",
    "\n",
    "class Mention(object):\n",
    "    def __init__(self, sentence, start, end, head, text, representative):\n",
    "        self.sentence = sentence\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "        self.head = head\n",
    "        self.text = text\n",
    "        self.representative = representative\n",
    "\n",
    "# {文番号: [Mention]}という辞書を作成する\n",
    "# 文の番号から置換する対象を引いてくることができる\n",
    "coreferences_dict = {}\n",
    "for coreference in root[0][1]:\n",
    "    represent = coreference.find('mention[@representative=\"true\"]')\n",
    "    for mention in coreference[:5]:\n",
    "        if \"representative\" in mention.attrib:\n",
    "            continue\n",
    "        mention_obj = Mention(mention[0].text, mention[1].text, mention[2].text, mention[3].text, mention[4].text, represent.find(\"text\").text)\n",
    "        if not mention_obj.sentence in coreferences_dict:\n",
    "            coreferences_dict[mention_obj.sentence] = [mention_obj]\n",
    "        else:\n",
    "            coreferences_dict[mention_obj.sentence].append(mention_obj)\n",
    "            \n",
    "for idx,sentence in enumerate(root[0][0][:5]):\n",
    "    # coreference_dictの文番号に値が格納されている場合、その文に置換対象が存在する\n",
    "    if str(idx+1) in coreferences_dict:\n",
    "        mentions = coreferences_dict[str(idx+1)]\n",
    "        words = []\n",
    "        for token in sentence[0]:\n",
    "            words.append(token[0].text)\n",
    "        sent = \" \".join(words)\n",
    "        for mention in mentions:\n",
    "            sent = sent.replace(mention.text, mention.representative + \"(\" + mention.text + \")\")\n",
    "        print(sent)\n",
    "    else:\n",
    "        words = []\n",
    "        for token in sentence[0]:\n",
    "            words.append(token[0].text)\n",
    "        print(\" \".join(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 57. 係り受け解析\n",
    "import pydot\n",
    "\n",
    "for idx, sentence in enumerate(root[0][0][:3]):\n",
    "    collapsed_dependencies = sentence.findall(\"dependencies[@type=\\\"collapsed-dependencies\\\"]\")\n",
    "    # print(collapsed_dependencies[0][0])\n",
    "    nodes = ['\"{}\" -> \"{}\" ;'.format(dep[1].text, dep[0].text) for dep in collapsed_dependencies[0]]\n",
    "    sentence_dot = \"digraph sentence{\" + \"\".join(nodes) + \"}\"\n",
    "    g = pydot.graph_from_dot_data(sentence_dot)\n",
    "    g[0].write_jpeg(\"./jpg/sentence{}.jpg\".format(idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "understanding\tenabling\tcomputers\n",
      "others\tinvolve\tgeneration\n"
     ]
    }
   ],
   "source": [
    "# 58. タプルの抽出\n",
    "\n",
    "for idx, sentence in enumerate(root[0][0][:3]):\n",
    "    collapsed_dependencies = sentence.findall(\"dependencies[@type=\\\"collapsed-dependencies\\\"]\")\n",
    "    nsubj_list = [dep for dep in collapsed_dependencies[0] if dep.attrib[\"type\"] == \"nsubj\"]\n",
    "    dobj_list = [dep for dep in collapsed_dependencies[0] if dep.attrib[\"type\"] == \"dobj\"]\n",
    "    if len(nsubj_list) == 0 or len(dobj_list) == 0:\n",
    "        continue\n",
    "    for nsubj in nsubj_list:\n",
    "        for dobj in dobj_list:\n",
    "            if nsubj[0].attrib[\"idx\"] == dobj[0].attrib[\"idx\"]:\n",
    "                print(\"{}\\t{}\\t{}\".format(nsubj[1].text, nsubj[0].text, dobj[1].text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 59. S式の解析\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
