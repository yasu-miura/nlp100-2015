{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 準備\n",
    "import CaboCha\n",
    "\n",
    "c = CaboCha.Parser()\n",
    "with open(\"./neko.txt\", \"r\") as rf:\n",
    "    with open(\"./neko.txt.cabocha\", \"w\") as wf:\n",
    "        for line in rf:\n",
    "            tree = c.parse(line.lstrip())\n",
    "            wf.write(tree.toString(CaboCha.FORMAT_LATTICE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "surface: 吾輩, base: 吾輩, pos: 名詞, pos1: 代名詞\n",
      "surface: は, base: は, pos: 助詞, pos1: 係助詞\n",
      "surface: 猫, base: 猫, pos: 名詞, pos1: 一般\n",
      "surface: で, base: だ, pos: 助動詞, pos1: *\n",
      "surface: ある, base: ある, pos: 助動詞, pos1: *\n",
      "surface: 。, base: 。, pos: 記号, pos1: 句点\n"
     ]
    }
   ],
   "source": [
    "# 40. 係り受け解析結果の読み込み（形態素）\n",
    "class Morph(object):\n",
    "    def __init__(self, surface, base, pos, pos1):\n",
    "        self.surface = surface\n",
    "        self.base = base\n",
    "        self.pos = pos\n",
    "        self.pos1 = pos1\n",
    "        \n",
    "    def __str__(self):\n",
    "        return \"surface: {}, base: {}, pos: {}, pos1: {}\".format(self.surface, self.base, self.pos, self.pos1)\n",
    "\n",
    "sentence_list = []\n",
    "sentence = []\n",
    "with open(\"./neko.txt.cabocha\", \"r\") as f:\n",
    "    for line in f:\n",
    "        if line == \"EOS\\n\":\n",
    "            sentence_list.append(sentence)\n",
    "            sentence = []\n",
    "            continue\n",
    "        elif line.split()[0] == \"*\":\n",
    "            continue\n",
    "        surface, features = line.split(\"\\t\")\n",
    "        morph = Morph(surface=surface, base=features.split(\",\")[6], pos=features.split(\",\")[0], pos1=features.split(\",\")[1])\n",
    "        sentence.append(morph)\n",
    "        \n",
    "for morph in sentence_list[2]:\n",
    "    print(str(morph))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文節番号 : 0, 文字列: この, 係り先文節番号 1\n",
      "文節番号 : 1, 文字列: 書生というのは, 係り先文節番号 7\n",
      "文節番号 : 2, 文字列: 時々, 係り先文節番号 4\n",
      "文節番号 : 3, 文字列: 我々を, 係り先文節番号 4\n",
      "文節番号 : 4, 文字列: 捕えて, 係り先文節番号 5\n",
      "文節番号 : 5, 文字列: 煮て, 係り先文節番号 6\n",
      "文節番号 : 6, 文字列: 食うという, 係り先文節番号 7\n",
      "文節番号 : 7, 文字列: 話である。, 係り先文節番号 -1\n"
     ]
    }
   ],
   "source": [
    "# 41. 係り受け解析結果の読み込み（文節・係り受け）\n",
    "class Chunk(object):\n",
    "    def __init__(self, morphs, dst, srcs):\n",
    "        self.morphs = morphs\n",
    "        self.dst = int(dst.strip(\"D\"))\n",
    "        self.srcs = int(srcs)\n",
    "        \n",
    "    def get_chunk_text(self):\n",
    "        text = \"\"\n",
    "        for morph in self.morphs:\n",
    "            if not morph.pos == \"記号\":\n",
    "                text += morph.surface\n",
    "        return text\n",
    "    \n",
    "    def has_noun(self):\n",
    "        for morph in self.morphs:\n",
    "            if morph.pos == \"名詞\":\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    def has_verb(self):\n",
    "        for morph in self.morphs:\n",
    "            if morph.pos == \"動詞\":\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    def has_particle(self):\n",
    "        for morph in self.morphs:\n",
    "            if morph.pos == \"助詞\":\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    def is_sahen_wo(self):\n",
    "        return len(self.morphs) == 2 and self.morphs[0].pos1 == \"サ変接続\" and self.morphs[1].base == \"を\"\n",
    "    \n",
    "    def get_last_particle(self):\n",
    "        return [_morph for _morph in self.morphs if _morph.pos == '助詞'][-1] if self.has_particle() else None\n",
    "    \n",
    "    def get_first_verb(self):\n",
    "        return [_morph for _morph in self.morphs if _morph.pos == '動詞'][0] if self.has_verb() else None\n",
    "    \n",
    "    \n",
    "    def replace_noun(self, alt):\n",
    "        for _morph in self.morphs:\n",
    "            if _morph.pos == '名詞':\n",
    "                _morph.surface = alt\n",
    "\n",
    "\n",
    "sentence_list = []\n",
    "sentence = []\n",
    "chunk = None\n",
    "with open(\"./neko.txt.cabocha\", \"r\") as f:\n",
    "    for line in f:\n",
    "        line_list = line.split()\n",
    "        if line_list[0] == \"*\":\n",
    "            if chunk is not None:\n",
    "                sentence.append(chunk)\n",
    "            chunk = Chunk(morphs=[], dst=line_list[2], srcs=line_list[1])\n",
    "        elif line_list[0] == \"EOS\":\n",
    "            if chunk is not None:\n",
    "                sentence.append(chunk)\n",
    "            if len(sentence) > 0:\n",
    "                sentence_list.append(sentence)\n",
    "            sentence = []\n",
    "            chunk = None\n",
    "            continue\n",
    "        else:\n",
    "            surface, features = line.split(\"\\t\")\n",
    "            morph = Morph(surface=surface, base=features.split(\",\")[6], pos=features.split(\",\")[0], pos1=features.split(\",\")[1])\n",
    "            chunk.morphs.append(morph)\n",
    "            \n",
    "for chunk in sentence_list[7]:\n",
    "    text = \"\"\n",
    "    for morph in chunk.morphs:\n",
    "        text += morph.surface\n",
    "    print(\"文節番号 : \" + str(chunk.srcs) + \", 文字列: \" + text + \", 係り先文節番号 \" + str(chunk.dst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====文=====\n",
      "しかも\t種族であったそうだ\n",
      "あとで\t聞くと\n",
      "聞くと\t種族であったそうだ\n",
      "それは\t種族であったそうだ\n",
      "書生という\t人間中で\n",
      "人間中で\t種族であったそうだ\n",
      "一番\t獰悪な\n",
      "獰悪な\t種族であったそうだ\n",
      "=====文=====\n",
      "この\t書生というのは\n",
      "書生というのは\t話である\n",
      "時々\t捕えて\n",
      "我々を\t捕えて\n",
      "捕えて\t煮て\n",
      "煮て\t食うという\n",
      "食うという\t話である\n"
     ]
    }
   ],
   "source": [
    "# 42. 係り元と係り先の文節の表示\n",
    "for sentence in sentence_list[6:8]:\n",
    "    print(\"=====文=====\")\n",
    "    for chunk in sentence:\n",
    "        if not chunk.dst == -1:\n",
    "            print(chunk.get_chunk_text() + \"\\t\" + sentence[chunk.dst].get_chunk_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====文=====\n",
      "吾輩は\t見た\n",
      "ここで\t始めて\n",
      "ものを\t見た\n",
      "=====文=====\n",
      "あとで\t聞くと\n",
      "=====文=====\n",
      "我々を\t捕えて\n"
     ]
    }
   ],
   "source": [
    "# 43. 名詞を含む文節が動詞を含む文節に係るものを抽出\n",
    "for sentence in sentence_list[5:8]:\n",
    "    print(\"=====文=====\")\n",
    "    for chunk in sentence:\n",
    "        if not chunk.dst == -1 and chunk.has_noun() and sentence[chunk.dst].has_verb():\n",
    "            print(chunk.get_chunk_text() + \"\\t\" + sentence[chunk.dst].get_chunk_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 44. 係り受け木の可視化\n",
    "import pydot\n",
    "for idx, sentence in enumerate(sentence_list[5:8]):\n",
    "    nodes = ['\"{}\"->\"{}\"; '.format(chunk.get_chunk_text(), sentence[chunk.dst].get_chunk_text()) for chunk in sentence if  not chunk.dst == -1]\n",
    "    sentence_dot = \"digraph sentence{\" + \"\".join(nodes) + \"}\"\n",
    "    g = pydot.graph_from_dot_data(sentence_dot)\n",
    "    g[0].write_jpeg(\"./jpg/sentence{}.jpg\".format(idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 45. 動詞の格パターンの抽出\n",
    "# (\"見る\", [\"は\", \"を\"])というタプルのリスト\n",
    "case_pattern_list = []\n",
    "for sentence in sentence_list:\n",
    "    for chunk in sentence:\n",
    "        if not chunk.has_verb():\n",
    "            continue\n",
    "        particles = [ch.get_last_particle().base for ch in sentence if ch.dst == chunk.srcs and ch.has_particle()]\n",
    "        if len(particles) == 0:\n",
    "            continue\n",
    "        particles.sort()\n",
    "        case_pattern = (chunk.get_first_verb().base, particles)\n",
    "        case_pattern_list.append(case_pattern)\n",
    "        \n",
    "with open(\"./case_patterns.txt\", \"w\") as f:\n",
    "    for case_pattern in case_pattern_list:\n",
    "        f.write(\"{}\\t{}\\n\".format(case_pattern[0], \" \".join(case_pattern[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "始める\tで\tここで\n",
      "見る\tは を\t吾輩は ものを\n",
      "聞く\tで\tあとで\n",
      "捕える\tを\t我々を\n",
      "煮る\tて\t捕えて\n",
      "食う\tて\t煮て\n"
     ]
    }
   ],
   "source": [
    "# 46. 動詞の格フレーム情報の抽出\n",
    "# (\"見る\", [\"は\", \"を\"], [\"吾輩は\", \"ものを\"])というタプルのリスト\n",
    "case_frame_pattern_list = []\n",
    "for sentence in sentence_list[5:8]:\n",
    "    for chunk in sentence:\n",
    "        if not chunk.has_verb():\n",
    "            continue\n",
    "        particles = [(ch.get_last_particle().base, ch.get_chunk_text()) for ch in sentence if ch.dst == chunk.srcs and ch.has_particle()]\n",
    "        if len(particles) == 0:\n",
    "            continue\n",
    "        sorted_particles = sorted(particles, key=lambda x:x[0])\n",
    "        particle_list, chunk_text_list = [], []\n",
    "        for t in sorted_particles:\n",
    "            particle_list.append(t[0])\n",
    "            chunk_text_list.append(t[1])\n",
    "        case_pattern = (chunk.get_first_verb().base, particle_list, chunk_text_list)\n",
    "        case_frame_pattern_list.append(case_pattern)\n",
    "        \n",
    "for cfp in case_frame_pattern_list:\n",
    "    print(cfp[0] + \"\\t\" + \" \".join(cfp[1]) + \"\\t\" + \" \".join(cfp[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "研究を続ける\tて て\t組織しまして 会合して\n",
      "同情を持つ\tに\t人物に\n",
      "質問をする\tが て\tしたが 見えて\n",
      "御馳走を致す\tに\t東風子に\n",
      "返事をする\tと に は\t及ばんさと 手紙に 主人は\n",
      "著述を居る\tて より\t依て 此間中より\n",
      "厚遇を受ける\tが まで\t猫が かくまで\n",
      "返事をする\tは\t下女は\n",
      "含嗽をやる\tで\t毎朝風呂場で\n"
     ]
    }
   ],
   "source": [
    "# 47. 機能動詞構文のマイニング\n",
    "sahen_frame_pattern_list = []\n",
    "for sentence in sentence_list[800:1000]:\n",
    "    for chunk in sentence:\n",
    "        if not chunk.has_verb():\n",
    "            continue\n",
    "        particles = [(ch.get_last_particle().base, ch) for ch in sentence if ch.dst == chunk.srcs and ch.has_particle()]\n",
    "        if len(particles) == 1:\n",
    "            continue\n",
    "        # 動詞に「サ変接続+を」が係るかチェック\n",
    "        has_sahen_wo = False\n",
    "        for particle in particles:\n",
    "            if  particle[1].is_sahen_wo():\n",
    "                has_sahen_wo = True\n",
    "                sahen_wo = particle[1].get_chunk_text()\n",
    "        if not has_sahen_wo:\n",
    "            continue\n",
    "        sorted_particles = sorted(particles, key=lambda x:x[0])\n",
    "        particle_list, chunk_text_list = [], []\n",
    "        for t in sorted_particles:\n",
    "            if t[1].get_chunk_text() == sahen_wo:\n",
    "                continue\n",
    "            particle_list.append(t[0])\n",
    "            chunk_text_list.append(t[1].get_chunk_text())\n",
    "        sahen_pattern = (sahen_wo + chunk.get_first_verb().base, particle_list, chunk_text_list)\n",
    "        sahen_frame_pattern_list.append(sahen_pattern)\n",
    "        \n",
    "for cfp in sahen_frame_pattern_list:\n",
    "    print(cfp[0] + \"\\t\" + \" \".join(cfp[1]) + \"\\t\" + \" \".join(cfp[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "吾輩は -> 見た\n",
      "ここで -> 始めて -> 人間という -> ものを -> 見た\n",
      "人間という -> ものを -> 見た\n",
      "ものを -> 見た\n",
      "あとで -> 聞くと -> 種族であったそうだ\n",
      "それは -> 種族であったそうだ\n",
      "書生という -> 人間中で -> 種族であったそうだ\n",
      "人間中で -> 種族であったそうだ\n",
      "一番 -> 獰悪な -> 種族であったそうだ\n",
      "獰悪な -> 種族であったそうだ\n",
      "書生というのは -> 話である\n",
      "我々を -> 捕えて -> 煮て -> 食うという -> 話である\n"
     ]
    }
   ],
   "source": [
    "# 48. 名詞から根へのパスの抽出\n",
    "def get_path_list(sentence, idx, paths):\n",
    "    if sentence[idx].dst != -1:\n",
    "        paths.append(sentence[idx])\n",
    "        get_path_list(sentence, sentence[idx].dst, paths)\n",
    "    else:\n",
    "        paths.append(sentence[idx])\n",
    "    return paths\n",
    "\n",
    "\n",
    "paths_list = []\n",
    "for sentence in sentence_list[5:8]:\n",
    "    for chunk in sentence:\n",
    "        if chunk.has_noun():\n",
    "            paths = get_path_list(sentence, chunk.srcs, [])\n",
    "            if len(paths) == 1:\n",
    "                continue\n",
    "            paths_list.append(paths)\n",
    "            \n",
    "for paths in paths_list:\n",
    "    print(\" -> \".join([ch.get_chunk_text() for ch in paths]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xは -> 見た | Yで -> 始めて -> Yという -> Yを -> 見た | 見た\n",
      "Xは -> 見た | Yという -> Yを -> 見た | 見た\n",
      "Xは -> 見た | Yを -> 見た | 見た\n",
      "Xで -> 始めて -> Yという\n",
      "Xで -> 始めて -> Yという -> Yを\n",
      "Xという -> Yを\n",
      "Xで -> 聞くと -> 種族であったそうだ | Yは -> 種族であったそうだ | 種族であったそうだ\n",
      "Xで -> 聞くと -> 種族であったそうだ | Yという -> 人間中で -> 種族であったそうだ | 種族であったそうだ\n",
      "Xで -> 聞くと -> 種族であったそうだ | YYで -> 種族であったそうだ | 種族であったそうだ\n",
      "Xで -> 聞くと -> 種族であったそうだ | Y -> 獰悪な -> 種族であったそうだ | 種族であったそうだ\n",
      "Xで -> 聞くと -> 種族であったそうだ | Yな -> 種族であったそうだ | 種族であったそうだ\n",
      "Xで -> 聞くと -> YであったYだ\n",
      "Xは -> YであったYだ | Yという -> YYで -> YであったYだ | YであったYだ\n",
      "Xは -> YであったYだ | YYで -> YであったYだ | YであったYだ\n",
      "Xは -> YであったYだ | Y -> Yな -> YであったYだ | YであったYだ\n",
      "Xは -> YであったYだ | Yな -> YであったYだ | YであったYだ\n",
      "Xは -> YであったYだ\n",
      "Xという -> YYで\n",
      "Xという -> YYで -> YであったYだ | Y -> Yな -> YであったYだ | YであったYだ\n",
      "Xという -> YYで -> YであったYだ | Yな -> YであったYだ | YであったYだ\n",
      "Xという -> YYで -> YであったYだ\n",
      "XXで -> YであったYだ | Y -> Yな -> YであったYだ | YであったYだ\n",
      "XXで -> YであったYだ | Yな -> YであったYだ | YであったYだ\n",
      "XXで -> YであったYだ\n",
      "X -> Yな\n",
      "X -> Yな -> YであったYだ\n",
      "Xな -> YであったYだ\n",
      "XというXは -> 話である | Yを -> 捕えて -> 煮て -> 食うという -> 話である | 話である\n",
      "XというXは -> Yである\n",
      "Xを -> 捕えて -> 煮て -> 食うという -> Yである\n"
     ]
    }
   ],
   "source": [
    "# 49. 名詞間の係り受けパスの抽出\n",
    "from itertools import combinations\n",
    "\n",
    "def noun_pairs(sen):\n",
    "    noun_chunk_list = [ch for ch in sen if ch.has_noun()]\n",
    "    return list(combinations(noun_chunk_list, 2))\n",
    "\n",
    "\n",
    "def common_chunk(path_i, path_j):\n",
    "    chunk_k = None\n",
    "    path_i = list(reversed(path_i))\n",
    "    path_j = list(reversed(path_j))\n",
    "    for idx, (c_i, c_j) in enumerate(zip(path_i, path_j)):\n",
    "        if c_i.srcs != c_j.srcs:\n",
    "            chunk_k = path_i[idx - 1]\n",
    "            break\n",
    "\n",
    "    return chunk_k\n",
    "\n",
    "\n",
    "for sentence in sentence_list[5:8]:\n",
    "    n_pairs = noun_pairs(sentence)\n",
    "    if len(n_pairs) == 0:\n",
    "        continue\n",
    "    \n",
    "    for n_pair in n_pairs:\n",
    "        chunk_i, chunk_j = n_pair\n",
    "        chunk_i.replace_noun('X')\n",
    "        chunk_j.replace_noun('Y')\n",
    "        \n",
    "        paths_i = get_path_list(sentence, chunk_i.srcs, [])\n",
    "        paths_j = get_path_list(sentence, chunk_j.srcs, [])\n",
    "        \n",
    "        if chunk_j in paths_i:\n",
    "            # 文節iiから構文木の根に至る経路上に文節jjが存在する場合\n",
    "            idx_j = paths_i.index(chunk_j)\n",
    "            print(\" -> \".join([ch.get_chunk_text() for ch in paths_i[:idx_j + 1]]))\n",
    "            \n",
    "        else:\n",
    "            # 文節iiと文節jjから構文木の根に至る経路上で共通の文節kkで交わる場合\n",
    "            chunk_k = common_chunk(paths_i, paths_j)\n",
    "            if chunk_k is None:\n",
    "                continue\n",
    "            idx_k_i = paths_i.index(chunk_k)\n",
    "            idx_k_j = paths_j.index(chunk_k)\n",
    "            print(\" | \".join([\" -> \".join([ch.get_chunk_text() for ch in paths_i[:idx_k_i + 1]]),\n",
    "                             \" -> \".join([ch.get_chunk_text() for ch in paths_j[:idx_k_j + 1]]),\n",
    "                             chunk_k.get_chunk_text()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
